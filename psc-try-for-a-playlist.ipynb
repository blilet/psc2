{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7376459,"sourceType":"datasetVersion","datasetId":4286363},{"sourceId":7376983,"sourceType":"datasetVersion","datasetId":4286763},{"sourceId":7377625,"sourceType":"datasetVersion","datasetId":4287228},{"sourceId":7419546,"sourceType":"datasetVersion","datasetId":4316548}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## The whole process of diarization, speakers selection and gender prediction applied to a playlist","metadata":{}},{"cell_type":"code","source":"!pip install pyannote.audio","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm\nimport torch\nfrom pyannote.audio import Pipeline\nimport json\n\nimport scipy\nimport csv\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nfrom tensorflow.keras.models import load_model\n\nimport glob\nimport shutil\nfrom pydub import AudioSegment\nimport librosa\nfrom useful_funtions import extract_feature, preprocess_audio, predict_gender","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-17T09:53:50.909380Z","iopub.execute_input":"2024-01-17T09:53:50.909918Z","iopub.status.idle":"2024-01-17T09:53:50.914358Z","shell.execute_reply.started":"2024-01-17T09:53:50.909874Z","shell.execute_reply":"2024-01-17T09:53:50.913492Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Diarization","metadata":{}},{"cell_type":"markdown","source":"## Utilization of Multiple Dictionaries\n\nIn this analysis, several dictionaries are employed to organize and process the audio data effectively:\n\n- **`Diarizations{}`**: \n  - Associates each `file_name` with its corresponding diarization, which is an `Annotation` object.\n\n- **`unique_speakers{}`**: \n  - For each `file_name`, associates a set of unique speakers identified in the audio file.\n\n- **`durations_conferences{}`**: \n  - Maps each `file_name` to a `Duration{}` dictionary. \n  - For each speaker in the audio file, `Duration{}` associates their total spoken duration.\n  - **Purpose**: This is particularly useful for determining the principal speaker in each conference.\n\n- **`longest_segments_conferences{}`**: \n  - For each `file_name`, associates a `longest_segments{}` dictionary.\n  - For each speaker in the audio file, `longest_segments{}` associates the duration of the longest spoken segment and the corresponding segment.\n  - **Purpose**: This is useful for extracting a reasonable subsegment for every speaker to predict their gender.\n","metadata":{}},{"cell_type":"markdown","source":"## Paths","metadata":{}},{"cell_type":"code","source":"# Path to the directory containing audio files\naudio_folder = '/kaggle/input/playlist2'\n\n# Path to the gender prediction model\nmodel_path =\"/kaggle/input/genderrec/model.h5\"\n\nmodel = load_model(model_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Auxiliary functions","metadata":{}},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef convert_mp3_to_wav(mp3_path, wav_path):\n    if not os.path.exists(wav_path):\n        cmd = ['ffmpeg', '-i', mp3_path, '-acodec', 'pcm_s16le', '-ar', '16000', wav_path]\n        subprocess.run(cmd, check=True)\n        \n        \ndef delete_file(file_path):\n    if os.path.exists(file_path):\n            os.remove(file_path)\n                  \n\ndef extract_subsegment(source_path, start_time, end_time, output_path):\n\n    audio = AudioSegment.from_mp3(source_path)\n\n    subsegment = audio[start_time:end_time]\n\n    subsegment.export(output_path, format=\"mp3\")","metadata":{"execution":{"iopub.status.busy":"2024-01-17T11:58:36.157446Z","iopub.execute_input":"2024-01-17T11:58:36.158268Z","iopub.status.idle":"2024-01-17T11:58:36.163495Z","shell.execute_reply.started":"2024-01-17T11:58:36.158235Z","shell.execute_reply":"2024-01-17T11:58:36.162644Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Diarization and extraction of different speakers in the audio files","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-3.1\",\n    use_auth_token=\"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\")\n\n# send pipeline to GPU (when available)\npipeline.to(torch.device(\"cuda\"))\n\n\n# Dictionnaire pour stocker les résultats de la diarisation\nDiarizations = {}\nunique_speakers={}\ndurations_conferences = {}\nlongest_segments_conferences = {}\n\n\n\n# Iterate over the MP3 files in the directory with a progress bar\nfor file in tqdm(os.listdir(audio_folder), desc=\"Processing audio files\"):\n    # Check if the file is an MP3 file\n    if file.endswith('.mp3'):\n        file_name = os.path.basename(file)\n        mp3_path = os.path.join(audio_folder, file)\n        wav_path = file_name[:-3] + \"wav\" \n        convert_mp3_to_wav(mp3_path, wav_path)\n        diarization = pipeline(wav_path)\n        Diarizations[file_name] = diarization\n        #Unique speakers\n        unique_speaker = set(label for turn,_, label in diarization.itertracks(yield_label=True))\n        unique_speakers[file_name] = unique_speaker\n        #Durations\n        Duration={}\n        #Longest segments\n        Longest_segment = {}\n        for speaker in unique_speaker:\n            longest_segment = None\n            longest_duration = 0\n            total_duration = 0\n\n            for segment, _, speaker_iter in diarization.itertracks(yield_label=True):\n                if speaker_iter == speaker:\n                    duration = segment.duration\n                    total_duration += duration\n                    if duration > longest_duration:\n                        longest_duration = duration\n                        longest_segment = segment\n\n                Duration[speaker] = total_duration\n                Longest_segment[speaker] = [longest_duration, longest_segment]\n        durations_conferences[file_name] = Duration\n        longest_segments_conferences[file_name] = Longest_segment\n\n        delete_file(wav_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T11:58:39.300987Z","iopub.execute_input":"2024-01-17T11:58:39.301700Z","iopub.status.idle":"2024-01-17T12:03:37.009107Z","shell.execute_reply.started":"2024-01-17T11:58:39.301659Z","shell.execute_reply":"2024-01-17T12:03:37.007666Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Processing audio files:   2%|▏         | 1/54 [04:56<4:21:46, 296.35s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m wav_path \u001b[38;5;241m=\u001b[39m file_name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     21\u001b[0m convert_mp3_to_wav(mp3_path, wav_path)\n\u001b[0;32m---> 22\u001b[0m diarization \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m Diarizations[file_name] \u001b[38;5;241m=\u001b[39m diarization\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyannote/audio/core/pipeline.py:325\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, file, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessors\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m ProtocolFile(file, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessors)\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyannote/audio/pipelines/speaker_diarization.py:523\u001b[0m, in \u001b[0;36mSpeakerDiarization.apply\u001b[0;34m(self, file, num_speakers, min_speakers, max_speakers, return_embeddings, hook)\u001b[0m\n\u001b[1;32m    520\u001b[0m     hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings)\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m#   shape: (num_chunks, local_num_speakers, dimension)\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m hard_clusters, _, centroids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclustering\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegmentations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinarized_segmentations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_speakers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_speakers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_speakers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <== for oracle clustering\u001b[39;49;00m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <== for oracle clustering\u001b[39;49;00m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# hard_clusters: (num_chunks, num_speakers)\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# centroids: (num_speakers, dimension)\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# number of detected clusters is the number of different speakers\u001b[39;00m\n\u001b[1;32m    536\u001b[0m num_different_speakers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(hard_clusters) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyannote/audio/pipelines/clustering.py:265\u001b[0m, in \u001b[0;36mBaseClustering.__call__\u001b[0;34m(self, embeddings, segmentations, num_clusters, min_clusters, max_clusters, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     centroids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_embeddings, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hard_clusters, soft_clusters, centroids\n\u001b[0;32m--> 265\u001b[0m train_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m hard_clusters, soft_clusters, centroids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_embeddings(\n\u001b[1;32m    273\u001b[0m     embeddings,\n\u001b[1;32m    274\u001b[0m     train_chunk_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m     constrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstrained_assignment,\n\u001b[1;32m    278\u001b[0m )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hard_clusters, soft_clusters, centroids\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyannote/audio/pipelines/clustering.py:365\u001b[0m, in \u001b[0;36mAgglomerativeClustering.cluster\u001b[0;34m(self, embeddings, min_clusters, max_clusters, num_clusters)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    364\u001b[0m         embeddings \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embeddings, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 365\u001b[0m     dendrogram: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[43mlinkage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# other methods work just fine with any metric\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     dendrogram: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m linkage(\n\u001b[1;32m    372\u001b[0m         embeddings, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[1;32m    373\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/cluster/hierarchy.py:1060\u001b[0m, in \u001b[0;36mlinkage\u001b[0;34m(y, method, metric, optimal_ordering)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(y \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(y, y\u001b[38;5;241m.\u001b[39mT):\n\u001b[1;32m   1057\u001b[0m             _warning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe symmetric non-negative hollow observation \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1058\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatrix looks suspiciously like an uncondensed \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1059\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1060\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mdistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`y` must be 1 or 2 dimensional.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/spatial/distance.py:2250\u001b[0m, in \u001b[0;36mpdist\u001b[0;34m(X, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2249\u001b[0m     pdist_fn \u001b[38;5;241m=\u001b[39m metric_info\u001b[38;5;241m.\u001b[39mpdist_func\n\u001b[0;32m-> 2250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2252\u001b[0m     metric_info \u001b[38;5;241m=\u001b[39m _TEST_METRICS\u001b[38;5;241m.\u001b[39mget(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Saving excerpts for each speaker in each audio file","metadata":{}},{"cell_type":"code","source":"audio_folder=\"/kaggle/input/playlist2\"\nSegmentsPerFile ={}\nfor file in tqdm(os.listdir(audio_folder), desc=\"Saving audios for each speaker in each audio file\"):\n    file_name = os.path.basename(file)\n    unique_speaker = unique_speakers[file_name]\n    Longest_segment = longest_segments_conferences[file_name]\n    print(Longest_segment)\n    \n    SegmentsPerSpeaker = {}\n    \n    for speaker in unique_speaker :\n        if Longest_segment[speaker][0]>3 :\n            print(speaker, file_name, Longest_segment[speaker][0],Longest_segment[speaker][1].start,Longest_segment[speaker][1].end)\n            start_time = Longest_segment[speaker][1].start * 1000\n            end_time = Longest_segment[speaker][1].end * 1000\n            extract_subsegment('/kaggle/input/playlist/' + file_name, start_time, end_time, file_name[:-4] +speaker+ '.mp3')\n            SegmentsPerSpeaker[speaker] = file_name[:-4] + speaker+'.mp3'\n\n    SegmentsPerFile[file_name] = SegmentsPerSpeaker","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gender prediction","metadata":{}},{"cell_type":"code","source":"conferences_info = []\n\nfor file in os.listdir(audio_folder):\n    file_name = os.path.basename(file)\n    unique_speaker = unique_speakers[file_name]\n    longest_segment = longest_segments_conferences[file_name]\n    speakers_duration = durations_conferences[file_name]\n\n    # Principal speaker is the one who talked the most\n    principal_speaker = max(speakers_duration, key=speakers_duration.get)\n\n    # Predict gender for principal speaker\n    principal_speaker_gender = predict_gender(principal_speaker, file_name, model)\n\n    # Collect data for each conference\n    conference_data = {\n        \"conference\": file_name,\n        \"principal_speaker\": principal_speaker,\n        \"principal_speaker_gender\": principal_speaker_gender,\n        \"number_of_interruptions\": len(unique_speaker) - 1,\n        \"interruptors\": []\n    }\n\n    # Process interruptors\n    for speaker in unique_speaker:\n        if speaker != principal_speaker:\n            try:\n                gender = predict_gender(speaker, file_name, model)\n                conference_data[\"interruptors\"].append({\n                    \"speaker\": speaker,\n                    \"gender\": gender\n                })\n            except FileNotFoundError:\n                conference_data[\"interruptors\"].append({\n                    \"speaker\": speaker,\n                    \"error\": \"Interruption was too short\"\n                })\n\n    conferences_info.append(conference_data)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T12:13:46.695815Z","iopub.execute_input":"2024-01-17T12:13:46.696598Z","iopub.status.idle":"2024-01-17T12:13:48.556826Z","shell.execute_reply.started":"2024-01-17T12:13:46.696567Z","shell.execute_reply":"2024-01-17T12:13:48.555495Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file)\n\u001b[1;32m     36\u001b[0m unique_speaker \u001b[38;5;241m=\u001b[39m unique_speakers[file_name]\n\u001b[0;32m---> 37\u001b[0m longest_segment \u001b[38;5;241m=\u001b[39m \u001b[43mlongest_segments_conferences\u001b[49m[file_name]\n\u001b[1;32m     38\u001b[0m speakers_duration \u001b[38;5;241m=\u001b[39m durations_conferences[file_name]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Principal speaker is the one who talked the most\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'longest_segments_conferences' is not defined"],"ename":"NameError","evalue":"name 'longest_segments_conferences' is not defined","output_type":"error"}]}]}