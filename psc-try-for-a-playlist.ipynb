{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7376459,"sourceType":"datasetVersion","datasetId":4286363},{"sourceId":7376983,"sourceType":"datasetVersion","datasetId":4286763},{"sourceId":7377625,"sourceType":"datasetVersion","datasetId":4287228},{"sourceId":7419546,"sourceType":"datasetVersion","datasetId":4316548}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## The whole process of diarization, speakers selection and gender prediction applied to a playlist","metadata":{}},{"cell_type":"code","source":"!pip install pyannote.audio","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm\nimport torch\nfrom pyannote.audio import Pipeline\nimport json\n\nimport scipy\nimport csv\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nfrom tensorflow.keras.models import load_model\n\nimport glob\nimport shutil\nfrom pydub import AudioSegment\nimport librosa\nfrom useful_funtions import extract_feature, preprocess_audio, predict_gender","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-17T09:53:50.909380Z","iopub.execute_input":"2024-01-17T09:53:50.909918Z","iopub.status.idle":"2024-01-17T09:53:50.914358Z","shell.execute_reply.started":"2024-01-17T09:53:50.909874Z","shell.execute_reply":"2024-01-17T09:53:50.913492Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Diarization","metadata":{}},{"cell_type":"markdown","source":"## Utilization of Multiple Dictionaries\n\nIn this analysis, several dictionaries are employed to organize and process the audio data effectively:\n\n- **`Diarizations{}`**: \n  - Associates each `file_name` with its corresponding diarization, which is an `Annotation` object.\n\n- **`unique_speakers{}`**: \n  - For each `file_name`, associates a set of unique speakers identified in the audio file.\n\n- **`durations_conferences{}`**: \n  - Maps each `file_name` to a `Duration{}` dictionary. \n  - For each speaker in the audio file, `Duration{}` associates their total spoken duration.\n  - **Purpose**: This is particularly useful for determining the principal speaker in each conference.\n\n- **`longest_segments_conferences{}`**: \n  - For each `file_name`, associates a `longest_segments{}` dictionary.\n  - For each speaker in the audio file, `longest_segments{}` associates the duration of the longest spoken segment and the corresponding segment.\n  - **Purpose**: This is useful for extracting a reasonable subsegment for every speaker to predict their gender.\n","metadata":{}},{"cell_type":"markdown","source":"## Paths","metadata":{}},{"cell_type":"code","source":"# Path to the directory containing audio files\naudio_folder = '/kaggle/input/playlist2'\n\n# Path to the gender prediction model\nmodel_path =\"/kaggle/input/genderrec/model.h5\"\n\nmodel = load_model(model_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Auxiliary functions","metadata":{}},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef convert_mp3_to_wav(mp3_path, wav_path):\n    if not os.path.exists(wav_path):\n        cmd = ['ffmpeg', '-i', mp3_path, '-acodec', 'pcm_s16le', '-ar', '16000', wav_path]\n        subprocess.run(cmd, check=True)\n        \n        \ndef delete_file(file_path):\n    if os.path.exists(file_path):\n            os.remove(file_path)\n                  \n\ndef extract_subsegment(source_path, start_time, end_time, output_path):\n\n    audio = AudioSegment.from_mp3(source_path)\n\n    subsegment = audio[start_time:end_time]\n\n    subsegment.export(output_path, format=\"mp3\")","metadata":{"execution":{"iopub.status.busy":"2024-01-17T11:58:36.157446Z","iopub.execute_input":"2024-01-17T11:58:36.158268Z","iopub.status.idle":"2024-01-17T11:58:36.163495Z","shell.execute_reply.started":"2024-01-17T11:58:36.158235Z","shell.execute_reply":"2024-01-17T11:58:36.162644Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Diarization and extraction of different speakers in the audio files","metadata":{}},{"cell_type":"markdown","source":"### Saving excerpts for each speaker in each audio file","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-3.1\",\n    use_auth_token=\"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\")\n\n# send pipeline to GPU (when available)\npipeline.to(torch.device(\"cuda\"))\n\n\n# Dictionnaire pour stocker les rÃ©sultats de la diarisation\nDiarizations = {}\nunique_speakers={}\ndurations_conferences = {}\nlongest_segments_conferences = {}\n\n\n\n# Iterate over the MP3 files in the directory with a progress bar\nfor file in tqdm(os.listdir(audio_folder), desc=\"Processing audio files\"):\n    # Check if the file is an MP3 file\n    if file.endswith('.mp3'):\n        file_name = os.path.basename(file)\n        mp3_path = os.path.join(audio_folder, file)\n        wav_path = file_name[:-3] + \"wav\" \n        convert_mp3_to_wav(mp3_path, wav_path)\n        diarization = pipeline(wav_path)\n        Diarizations[file_name] = diarization\n        #Unique speakers\n        unique_speaker = set(label for turn,_, label in diarization.itertracks(yield_label=True))\n        unique_speakers[file_name] = unique_speaker\n        #Durations\n        Duration={}\n        #Longest segments\n        Longest_segment = {}\n        for speaker in unique_speaker:\n            longest_segment = None\n            longest_duration = 0\n            total_duration = 0\n\n            for segment, _, speaker_iter in diarization.itertracks(yield_label=True):\n                if speaker_iter == speaker:\n                    duration = segment.duration\n                    total_duration += duration\n                    if duration > longest_duration:\n                        longest_duration = duration\n                        longest_segment = segment\n\n                Duration[speaker] = total_duration\n                Longest_segment[speaker] = [longest_duration, longest_segment]\n        durations_conferences[file_name] = Duration\n        longest_segments_conferences[file_name] = Longest_segment\n\n        delete_file(wav_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_folder=\"/kaggle/input/playlist2\"\nSegmentsPerFile ={}\nfor file in tqdm(os.listdir(audio_folder), desc=\"Saving audios for each speaker in each audio file\"):\n    file_name = os.path.basename(file)\n    unique_speaker = unique_speakers[file_name]\n    Longest_segment = longest_segments_conferences[file_name]\n    print(Longest_segment)\n    \n    SegmentsPerSpeaker = {}\n    \n    for speaker in unique_speaker :\n        if Longest_segment[speaker][0]>3 :\n            print(speaker, file_name, Longest_segment[speaker][0],Longest_segment[speaker][1].start,Longest_segment[speaker][1].end)\n            start_time = Longest_segment[speaker][1].start * 1000\n            end_time = Longest_segment[speaker][1].end * 1000\n            extract_subsegment('/kaggle/input/playlist/' + file_name, start_time, end_time, file_name[:-4] +speaker+ '.mp3')\n            SegmentsPerSpeaker[speaker] = file_name[:-4] + speaker+'.mp3'\n\n    SegmentsPerFile[file_name] = SegmentsPerSpeaker","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gender prediction","metadata":{}},{"cell_type":"code","source":"conferences_info = []\n\nfor file in os.listdir(audio_folder):\n    file_name = os.path.basename(file)\n    unique_speaker = unique_speakers[file_name]\n    longest_segment = longest_segments_conferences[file_name]\n    speakers_duration = durations_conferences[file_name]\n\n    # Principal speaker is the one who talked the most\n    principal_speaker = max(speakers_duration, key=speakers_duration.get)\n\n    # Predict gender for principal speaker\n    principal_speaker_gender = predict_gender(principal_speaker, file_name, model)\n\n    # Collect data for each conference\n    conference_data = {\n        \"conference\": file_name,\n        \"principal_speaker\": principal_speaker,\n        \"principal_speaker_gender\": principal_speaker_gender,\n        \"number_of_interruptions\": len(unique_speaker) - 1,\n        \"interruptors\": []\n    }\n\n    # Process interruptors\n    for speaker in unique_speaker:\n        if speaker != principal_speaker:\n            try:\n                gender = predict_gender(speaker, file_name, model)\n                conference_data[\"interruptors\"].append({\n                    \"speaker\": speaker,\n                    \"gender\": gender\n                })\n            except FileNotFoundError:\n                conference_data[\"interruptors\"].append({\n                    \"speaker\": speaker,\n                    \"error\": \"Interruption was too short\"\n                })\n\n    conferences_info.append(conference_data)\n\n","metadata":{},"execution_count":null,"outputs":[]}]}