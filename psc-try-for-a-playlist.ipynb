{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7376459,"sourceType":"datasetVersion","datasetId":4286363},{"sourceId":7376983,"sourceType":"datasetVersion","datasetId":4286763},{"sourceId":7377625,"sourceType":"datasetVersion","datasetId":4287228}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-10T18:48:08.875292Z","iopub.execute_input":"2024-01-10T18:48:08.875668Z","iopub.status.idle":"2024-01-10T18:48:08.880913Z","shell.execute_reply.started":"2024-01-10T18:48:08.875630Z","shell.execute_reply":"2024-01-10T18:48:08.879911Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## First try on a Playlist of conferences","metadata":{}},{"cell_type":"markdown","source":"## Utilization of Multiple Dictionaries\n\nIn this analysis, several dictionaries are employed to organize and process the audio data effectively:\n\n- **`Diarizations{}`**: \n  - Associates each `file_name` with its corresponding diarization, which is an `Annotation` object.\n\n- **`unique_speakers{}`**: \n  - For each `file_name`, associates a set of unique speakers identified in the audio file.\n\n- **`durations_conferences{}`**: \n  - Maps each `file_name` to a `Duration{}` dictionary. \n  - For each speaker in the audio file, `Duration{}` associates their total spoken duration.\n  - **Purpose**: This is particularly useful for determining the principal speaker in each conference.\n\n- **`longest_segments_conferences{}`**: \n  - For each `file_name`, associates a `longest_segments{}` dictionary.\n  - For each speaker in the audio file, `longest_segments{}` associates the duration of the longest spoken segment and the corresponding segment.\n  - **Purpose**: This is useful for extracting a reasonable subsegment for every speaker to predict their gender.\n","metadata":{}},{"cell_type":"code","source":"!pip install pyannote.audio","metadata":{"execution":{"iopub.status.busy":"2024-01-10T16:34:15.810476Z","iopub.execute_input":"2024-01-10T16:34:15.811234Z","iopub.status.idle":"2024-01-10T16:34:43.503484Z","shell.execute_reply.started":"2024-01-10T16:34:15.811200Z","shell.execute_reply":"2024-01-10T16:34:43.502524Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pyannote.audio\n  Obtaining dependency information for pyannote.audio from https://files.pythonhosted.org/packages/f5/11/611c32f7b7894ba588ade502525d0130f3e731d15f925e9f2a1ae66c8680/pyannote.audio-3.1.1-py2.py3-none-any.whl.metadata\n  Downloading pyannote.audio-3.1.1-py2.py3-none-any.whl.metadata (9.3 kB)\nCollecting asteroid-filterbanks>=0.4 (from pyannote.audio)\n  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\nCollecting einops>=0.6.0 (from pyannote.audio)\n  Obtaining dependency information for einops>=0.6.0 from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: huggingface-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from pyannote.audio) (0.19.4)\nCollecting lightning>=2.0.1 (from pyannote.audio)\n  Obtaining dependency information for lightning>=2.0.1 from https://files.pythonhosted.org/packages/8c/a1/b2a6c33675510bc3e1ca6d010b244ac0dd9c81fc1723a37e7491aa586041/lightning-2.1.3-py3-none-any.whl.metadata\n  Downloading lightning-2.1.3-py3-none-any.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting omegaconf<3.0,>=2.1 (from pyannote.audio)\n  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyannote.core>=5.0.0 (from pyannote.audio)\n  Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyannote.database>=5.0.1 (from pyannote.audio)\n  Downloading pyannote.database-5.0.1-py3-none-any.whl (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyannote.metrics>=3.2 (from pyannote.audio)\n  Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyannote.pipeline>=3.0.1 (from pyannote.audio)\n  Obtaining dependency information for pyannote.pipeline>=3.0.1 from https://files.pythonhosted.org/packages/83/42/1bf7cbf061ed05c580bfb63bffdd3f3474cbd5c02bee4fac518eea9e9d9e/pyannote.pipeline-3.0.1-py3-none-any.whl.metadata\n  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\nCollecting pytorch-metric-learning>=2.1.0 (from pyannote.audio)\n  Obtaining dependency information for pytorch-metric-learning>=2.1.0 from https://files.pythonhosted.org/packages/2c/22/d48f4fd53e0636ba2ccf862b6af842097e1731702a907102762dff228e47/pytorch_metric_learning-2.4.1-py3-none-any.whl.metadata\n  Downloading pytorch_metric_learning-2.4.1-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: rich>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from pyannote.audio) (13.5.2)\nRequirement already satisfied: semver>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from pyannote.audio) (3.0.2)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from pyannote.audio) (0.12.1)\nCollecting speechbrain>=0.5.14 (from pyannote.audio)\n  Obtaining dependency information for speechbrain>=0.5.14 from https://files.pythonhosted.org/packages/30/ff/9629de25786cf6ce38b7754b352a9904ce8c90b22a127d9d0d9c45ef5693/speechbrain-0.5.16-py3-none-any.whl.metadata\n  Downloading speechbrain-0.5.16-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: tensorboardX>=2.6 in /opt/conda/lib/python3.10/site-packages (from pyannote.audio) (2.6.2.2)\nRequirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pyannote.audio) (2.0.0)\nCollecting torch-audiomentations>=0.11.0 (from pyannote.audio)\n  Downloading torch_audiomentations-0.11.0-py3-none-any.whl (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torchaudio>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pyannote.audio) (2.0.1)\nRequirement already satisfied: torchmetrics>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from pyannote.audio) (1.2.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (1.24.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (21.3)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning>=2.0.1->pyannote.audio) (0.10.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning>=2.0.1->pyannote.audio) (2.1.2)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf<3.0,>=2.1->pyannote.audio)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: sortedcontainers>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\nRequirement already satisfied: scipy>=1.1 in /opt/conda/lib/python3.10/site-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.11.4)\nRequirement already satisfied: pandas>=0.19 in /opt/conda/lib/python3.10/site-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.0.3)\nRequirement already satisfied: typer[all]>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.9.0)\nRequirement already satisfied: scikit-learn>=0.17.1 in /opt/conda/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.2.2)\nRequirement already satisfied: docopt>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.6.2)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\nRequirement already satisfied: matplotlib>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.7.4)\nRequirement already satisfied: sympy>=1.1 in /opt/conda/lib/python3.10/site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.12)\nRequirement already satisfied: optuna>=3.1 in /opt/conda/lib/python3.10/site-packages (from pyannote.pipeline>=3.0.1->pyannote.audio) (3.5.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=12.0.0->pyannote.audio) (2.16.1)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->pyannote.audio) (1.15.1)\nCollecting hyperpyyaml (from speechbrain>=0.5.14->pyannote.audio)\n  Obtaining dependency information for hyperpyyaml from https://files.pythonhosted.org/packages/33/c9/751b6401887f4b50f9307cc1e53d287b3dc77c375c126aeb6335aff73ccb/HyperPyYAML-1.2.2-py3-none-any.whl.metadata\n  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from speechbrain>=0.5.14->pyannote.audio) (1.3.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from speechbrain>=0.5.14->pyannote.audio) (0.1.99)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX>=2.6->pyannote.audio) (3.20.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->pyannote.audio) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->pyannote.audio) (3.1.2)\nCollecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio)\n  Downloading julius-0.2.7.tar.gz (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: librosa>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.10.1)\nCollecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio)\n  Downloading torch_pitch_shift-1.2.4-py3-none-any.whl (4.9 kB)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.21)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (3.8.5)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (3.0.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.57.1)\nRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (1.8.0)\nRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.3.7)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (1.0.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning>=2.0.1->pyannote.audio) (68.1.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (10.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.8.2)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.13.0)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (6.8.0)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.20)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2023.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.2.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\nCollecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio)\n  Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio) (8.1.7)\nRequirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio) (0.4.6)\nRequirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\nRequirement already satisfied: ruamel.yaml>=0.17.28 in /opt/conda/lib/python3.10/site-packages (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio) (0.17.32)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->pyannote.audio) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2023.11.17)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.3.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.3.0)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.40.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.16.0)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio) (0.2.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.2)\nDownloading pyannote.audio-3.1.1-py2.py3-none-any.whl (208 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning-2.1.3-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\nDownloading pytorch_metric_learning-2.4.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading speechbrain-0.5.16-py3-none-any.whl (630 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.6/630.6 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: antlr4-python3-runtime, julius\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=3b544368321fa8dfdbc8aee426595b3d20ee2c4d104a0c32df8457e6dd428738\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for julius (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21868 sha256=6f2bf6f256117d781977105dcf9d64c4ad81bf846fa6a5abd166c173cc21a0ac\n  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\nSuccessfully built antlr4-python3-runtime julius\nInstalling collected packages: primePy, antlr4-python3-runtime, omegaconf, einops, pyannote.core, hyperpyyaml, pytorch-metric-learning, julius, asteroid-filterbanks, torch-pitch-shift, speechbrain, pyannote.database, torch-audiomentations, pyannote.pipeline, pyannote.metrics, lightning, pyannote.audio\nSuccessfully installed antlr4-python3-runtime-4.9.3 asteroid-filterbanks-0.4.0 einops-0.7.0 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.1.3 omegaconf-2.3.0 primePy-1.3 pyannote.audio-3.1.1 pyannote.core-5.0.0 pyannote.database-5.0.1 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-metric-learning-2.4.1 speechbrain-0.5.16 torch-audiomentations-0.11.0 torch-pitch-shift-1.2.4\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyannote.audio import Pipeline","metadata":{"execution":{"iopub.status.busy":"2024-01-10T16:34:57.475606Z","iopub.execute_input":"2024-01-10T16:34:57.476486Z","iopub.status.idle":"2024-01-10T16:35:09.362047Z","shell.execute_reply.started":"2024-01-10T16:34:57.476448Z","shell.execute_reply":"2024-01-10T16:35:09.360981Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from pydub import AudioSegment\n\ndef convert_mp3_to_wav(mp3_path, wav_path):\n    \"\"\"\n    Convertit un fichier MP3 en WAV.\n\n    :param mp3_path: Chemin du fichier MP3 source.\n    :param wav_path: Chemin du fichier WAV de sortie.\n    \"\"\"\n    # Charger le fichier audio MP3\n    audio = AudioSegment.from_mp3(mp3_path)\n\n    # Exporter en tant que fichier WAV\n    audio.export(wav_path, format=\"wav\")","metadata":{"execution":{"iopub.status.busy":"2024-01-10T16:35:09.363850Z","iopub.execute_input":"2024-01-10T16:35:09.364508Z","iopub.status.idle":"2024-01-10T16:35:09.372184Z","shell.execute_reply.started":"2024-01-10T16:35:09.364475Z","shell.execute_reply":"2024-01-10T16:35:09.371046Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-10T16:35:09.373419Z","iopub.execute_input":"2024-01-10T16:35:09.373740Z","iopub.status.idle":"2024-01-10T16:35:09.384138Z","shell.execute_reply.started":"2024-01-10T16:35:09.373704Z","shell.execute_reply":"2024-01-10T16:35:09.382759Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import os\n\n\npipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization-3.1\",\n    use_auth_token=\"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\")\n\n# send pipeline to GPU (when available)\nimport torch\npipeline.to(torch.device(\"cuda\"))\n\n\n# Path to the directory containing audio files\naudio_folder = '/kaggle/input/playlist'\n\n# Dictionnaire pour stocker les résultats de la diarisation\nDiarizations = {}\n\n# Iterate over the MP3 files in the directory with a progress bar\nfor file in tqdm(os.listdir(audio_folder), desc=\"Processing audio files\"):\n    # Check if the file is an MP3 file\n    if file.endswith('.mp3'):\n        file_name = os.path.basename(file)\n        mp3_path = os.path.join(audio_folder, file)\n        wav_path = file_name[:-3] + \"wav\" \n        convert_mp3_to_wav(mp3_path, wav_path)\n        diarization = pipeline(wav_path)\n        Diarizations[file_name] = diarization\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T17:07:09.522016Z","iopub.execute_input":"2024-01-10T17:07:09.522378Z","iopub.status.idle":"2024-01-10T17:34:31.814288Z","shell.execute_reply.started":"2024-01-10T17:07:09.522352Z","shell.execute_reply":"2024-01-10T17:34:31.813132Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Processing audio files: 100%|██████████| 13/13 [27:21<00:00, 126.28s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(Diarizations)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T17:35:36.341170Z","iopub.execute_input":"2024-01-10T17:35:36.341533Z","iopub.status.idle":"2024-01-10T17:35:36.346652Z","shell.execute_reply.started":"2024-01-10T17:35:36.341505Z","shell.execute_reply":"2024-01-10T17:35:36.345780Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"{'Cryo-EM Workshop - Ayelet Heimowitz .mp3': <pyannote.core.annotation.Annotation object at 0x7f489e7a9450>, 'Cryo-EM Workshop - Jose Maria Carazo.mp3': <pyannote.core.annotation.Annotation object at 0x7f489e7a9f90>, 'Flatiron Wide Algorithms and Mathematics - Erik Thiede .mp3': <pyannote.core.annotation.Annotation object at 0x7f489e850490>, 'Flatiron Wide Algorithms and Mathematics - Miles Stoudenmire .mp3': <pyannote.core.annotation.Annotation object at 0x7f489e3f3850>, 'Flatiron Wide Algorithms and Mathematics - Ashley Villar .mp3': <pyannote.core.annotation.Annotation object at 0x7f489e8f04c0>, 'Cryo-EM Workshop - Bridget Carragher .mp3': <pyannote.core.annotation.Annotation object at 0x7f489e69e110>, 'Cryo-EM Workshop - Joakim Anden.mp3': <pyannote.core.annotation.Annotation object at 0x7f489e69fc70>, 'Flatiron Wide Algorithms and Mathematics - Shirley Ho.mp3': <pyannote.core.annotation.Annotation object at 0x7f489e335630>, 'Cryo-EM Workshop - Erik Lindahl .mp3': <pyannote.core.annotation.Annotation object at 0x7f489e337130>, 'Cryo-EM Workshop - Joachim Frank.mp3': <pyannote.core.annotation.Annotation object at 0x7f489e425f90>, 'Flatiron Wide Algorithms and Mathematics - Risi Kondor.mp3': <pyannote.core.annotation.Annotation object at 0x7f489e21baf0>, 'Flatiron Wide Algorithms and Mathematics - Bob Carpenter.mp3': <pyannote.core.annotation.Annotation object at 0x7f489e148a30>, 'Cryo-EM Workshop - Roy Lederman.mp3': <pyannote.core.annotation.Annotation object at 0x7f489e6f99f0>}\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_speakers={}\nfor file in os.listdir(audio_folder):\n    file_name =os.path.basename(file)\n    diarization = Diarizations[file_name]\n    unique_speaker = set(label for turn,_, label in diarization.itertracks(yield_label=True))\n    unique_speakers[file_name] = unique_speaker","metadata":{"execution":{"iopub.status.busy":"2024-01-10T17:35:39.650521Z","iopub.execute_input":"2024-01-10T17:35:39.651444Z","iopub.status.idle":"2024-01-10T17:35:39.668154Z","shell.execute_reply.started":"2024-01-10T17:35:39.651408Z","shell.execute_reply":"2024-01-10T17:35:39.667199Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Total speaking duration for each speaker -> can be used to identify the main speaker\n# Extract the longest spoken segment by each speaker for gender recognition\n# Issue: what if the maximum speaking duration is <5 seconds? -> consider merging segments\n# In parallel, the gender of the speaker can be detected\ndurations_conferences = {}\nlongest_segments_conferences = {}\n\n\nfor file in tqdm(os.listdir(audio_folder), desc=\"Saving durations,and longest segments for each audio file\"):\n    file_name = os.path.basename(file)\n    diarization = Diarizations[file_name]\n    unique_speaker = unique_speakers[file_name]\n    Duration={}\n    Longest_segment = {}\n    for speaker in unique_speaker:\n        longest_segment = None\n        longest_duration = 0\n        total_duration = 0\n\n        for segment, _, speaker_iter in diarization.itertracks(yield_label=True):\n            if speaker_iter == speaker:\n                duration = segment.duration\n                total_duration += duration\n                if duration > longest_duration:\n                    longest_duration = duration\n                    longest_segment = segment\n\n            Duration[speaker] = total_duration\n            Longest_segment[speaker] = [longest_duration, longest_segment]\n    durations_conferences[file_name] = Duration\n    longest_segments_conferences[file_name] = Longest_segment\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:13:16.120076Z","iopub.execute_input":"2024-01-10T18:13:16.120471Z","iopub.status.idle":"2024-01-10T18:13:16.163376Z","shell.execute_reply.started":"2024-01-10T18:13:16.120439Z","shell.execute_reply":"2024-01-10T18:13:16.162540Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Saving durations,and longest segments for each audio file: 100%|██████████| 13/13 [00:00<00:00, 429.96it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(longest_segments_conferences[\"Cryo-EM Workshop - Joakim Anden.mp3\"])","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:13:21.198790Z","iopub.execute_input":"2024-01-10T18:13:21.199132Z","iopub.status.idle":"2024-01-10T18:13:21.203977Z","shell.execute_reply.started":"2024-01-10T18:13:21.199105Z","shell.execute_reply":"2024-01-10T18:13:21.203056Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"{'SPEAKER_01': [0.3904923599320682, <Segment(1375.41, 1375.8)>], 'SPEAKER_00': [150.0, <Segment(1342.25, 1492.25)>]}\n","output_type":"stream"}]},{"cell_type":"code","source":"#Extract a sub-audio segment from a larger audio file given specific start and end times\n\ndef extract_subsegment(source_path, start_time, end_time, output_path):\n    \"\"\"\n    Extrait un sous-segment d'un fichier audio MP3.\n\n    :param source_path: Chemin du fichier audio source MP3.\n    :param start_time: Temps de début du sous-segment en millisecondes.\n    :param end_time: Temps de fin du sous-segment en millisecondes.\n    :param output_path: Chemin du fichier MP3 de sortie.\n    \"\"\"\n    # Charger le fichier audio MP3\n    audio = AudioSegment.from_mp3(source_path)\n\n    # Extraire le sous-segment\n    subsegment = audio[start_time:end_time]\n\n    # Exporter le sous-segment en tant que fichier MP3\n    subsegment.export(output_path, format=\"mp3\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:13:25.117850Z","iopub.execute_input":"2024-01-10T18:13:25.118851Z","iopub.status.idle":"2024-01-10T18:13:25.125203Z","shell.execute_reply.started":"2024-01-10T18:13:25.118806Z","shell.execute_reply":"2024-01-10T18:13:25.124046Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_folder=\"/kaggle/input/playlist\"","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:13:29.830487Z","iopub.execute_input":"2024-01-10T18:13:29.830862Z","iopub.status.idle":"2024-01-10T18:13:29.835210Z","shell.execute_reply.started":"2024-01-10T18:13:29.830831Z","shell.execute_reply":"2024-01-10T18:13:29.834166Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"SegmentsPerFile ={}\nfor file in tqdm(os.listdir(audio_folder), desc=\"Saving audios for each speaker in each audio file\"):\n    file_name = os.path.basename(file)\n    unique_speaker = unique_speakers[file_name]\n    Longest_segment = longest_segments_conferences[file_name]\n    print(Longest_segment)\n    \n    SegmentsPerSpeaker = {}\n    \n    for speaker in unique_speaker :\n        if Longest_segment[speaker][0]>3 :\n            print(speaker, file_name, Longest_segment[speaker][0],Longest_segment[speaker][1].start,Longest_segment[speaker][1].end)\n            start_time = Longest_segment[speaker][1].start * 1000\n            end_time = Longest_segment[speaker][1].end * 1000\n            extract_subsegment('/kaggle/input/playlist/' + file_name, start_time, end_time, file_name[:-4] +speaker+ '.mp3')\n            SegmentsPerSpeaker[speaker] = file_name[:-4] + speaker+'.mp3'\n\n    SegmentsPerFile[file_name] = SegmentsPerSpeaker","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:13:31.526642Z","iopub.execute_input":"2024-01-10T18:13:31.527491Z","iopub.status.idle":"2024-01-10T18:16:41.037762Z","shell.execute_reply.started":"2024-01-10T18:13:31.527453Z","shell.execute_reply":"2024-01-10T18:16:41.036777Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"Saving audios for each speaker in each audio file:   0%|          | 0/13 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_00': [24.65195246179968, <Segment(117.275, 141.927)>]}\nSPEAKER_00 Cryo-EM Workshop - Ayelet Heimowitz .mp3 24.65195246179968 117.27504244482174 141.92699490662142\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:   8%|▊         | 1/13 [00:04<00:48,  4.02s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [0.37351443123952777, <Segment(1051.2, 1051.57)>], 'SPEAKER_00': [182.44482173174902, <Segment(888.26, 1070.7)>]}\nSPEAKER_00 Cryo-EM Workshop - Jose Maria Carazo.mp3 182.44482173174902 888.2597623089982 1070.7045840407472\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  15%|█▌        | 2/13 [00:13<01:22,  7.49s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [7.062818336162763, <Segment(1443.95, 1451.01)>], 'SPEAKER_00': [9.575551782682396, <Segment(1518.67, 1528.24)>]}\nSPEAKER_01 Flatiron Wide Algorithms and Mathematics - Erik Thiede .mp3 7.062818336162763 1443.947368421053 1451.0101867572157\nSPEAKER_00 Flatiron Wide Algorithms and Mathematics - Erik Thiede .mp3 9.575551782682396 1518.6672325976233 1528.2427843803057\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  23%|██▎       | 3/13 [00:22<01:21,  8.18s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [15.449915110356415, <Segment(2229.62, 2245.07)>], 'SPEAKER_03': [12.071307300509488, <Segment(1646.41, 1658.48)>], 'SPEAKER_00': [9.949066213921924, <Segment(2313.95, 2323.9)>], 'SPEAKER_02': [66.62139219015282, <Segment(37.0543, 103.676)>], 'SPEAKER_05': [5.95925297113763, <Segment(3189.07, 3195.03)>], 'SPEAKER_04': [11.341256366723428, <Segment(2363.46, 2374.8)>]}\nSPEAKER_01 Flatiron Wide Algorithms and Mathematics - Miles Stoudenmire .mp3 15.449915110356415 2229.617996604414 2245.0679117147706\nSPEAKER_03 Flatiron Wide Algorithms and Mathematics - Miles Stoudenmire .mp3 12.071307300509488 1646.409168081494 1658.4804753820035\nSPEAKER_00 Flatiron Wide Algorithms and Mathematics - Miles Stoudenmire .mp3 9.949066213921924 2313.9473684210525 2323.8964346349744\nSPEAKER_02 Flatiron Wide Algorithms and Mathematics - Miles Stoudenmire .mp3 66.62139219015282 37.054329371816635 103.67572156196945\nSPEAKER_05 Flatiron Wide Algorithms and Mathematics - Miles Stoudenmire .mp3 5.95925297113763 3189.074702886248 3195.0339558573855\nSPEAKER_04 Flatiron Wide Algorithms and Mathematics - Miles Stoudenmire .mp3 11.341256366723428 2363.4550084889643 2374.7962648556877\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  31%|███       | 4/13 [01:22<04:17, 28.61s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_00': [19.11714770797971, <Segment(910.823, 929.941)>]}\nSPEAKER_00 Flatiron Wide Algorithms and Mathematics - Ashley Villar .mp3 19.11714770797971 910.8234295415959 929.9405772495757\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  38%|███▊      | 5/13 [01:26<02:38, 19.77s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_00': [36.010186757215706, <Segment(1015.08, 1051.1)>]}\nSPEAKER_00 Cryo-EM Workshop - Bridget Carragher .mp3 36.010186757215706 1015.0848896434636 1051.0950764006793\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  46%|████▌     | 6/13 [01:33<01:47, 15.37s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [0.3904923599320682, <Segment(1375.41, 1375.8)>], 'SPEAKER_00': [150.0, <Segment(1342.25, 1492.25)>]}\nSPEAKER_00 Cryo-EM Workshop - Joakim Anden.mp3 150.0 1342.249575551783 1492.249575551783\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  54%|█████▍    | 7/13 [01:40<01:15, 12.55s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [5.008488964346725, <Segment(1136.38, 1141.38)>], 'SPEAKER_02': [5.127334465194963, <Segment(1153.96, 1159.09)>], 'SPEAKER_00': [6.19694397283547, <Segment(1106.31, 1112.5)>], 'SPEAKER_03': [16.332767402376902, <Segment(194.491, 210.823)>]}\nSPEAKER_01 Flatiron Wide Algorithms and Mathematics - Shirley Ho.mp3 5.008488964346725 1136.3752122241085 1141.3837011884552\nSPEAKER_02 Flatiron Wide Algorithms and Mathematics - Shirley Ho.mp3 5.127334465194963 1153.9643463497455 1159.0916808149404\nSPEAKER_00 Flatiron Wide Algorithms and Mathematics - Shirley Ho.mp3 6.19694397283547 1106.307300509338 1112.5042444821734\nSPEAKER_03 Flatiron Wide Algorithms and Mathematics - Shirley Ho.mp3 16.332767402376902 194.49066213921904 210.82342954159594\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  62%|██████▏   | 8/13 [01:57<01:09, 13.84s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [0.9507640067911609, <Segment(136.664, 137.615)>], 'SPEAKER_00': [96.74023769100131, <Segment(1675.36, 1772.1)>]}\nSPEAKER_00 Cryo-EM Workshop - Erik Lindahl .mp3 96.74023769100131 1675.356536502547 1772.0967741935483\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  69%|██████▉   | 9/13 [02:04<00:46, 11.67s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [0.3565365025465326, <Segment(1133.66, 1134.02)>], 'SPEAKER_00': [125.33106960950772, <Segment(747.886, 873.217)>]}\nSPEAKER_00 Cryo-EM Workshop - Joachim Frank.mp3 125.33106960950772 747.8862478777589 873.2173174872667\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  77%|███████▋  | 10/13 [02:12<00:31, 10.59s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [8.896434634974412, <Segment(1603.78, 1612.67)>], 'SPEAKER_00': [71.59592529711378, <Segment(66.0017, 137.598)>]}\nSPEAKER_01 Flatiron Wide Algorithms and Mathematics - Risi Kondor.mp3 8.896434634974412 1603.7775891341257 1612.6740237691001\nSPEAKER_00 Flatiron Wide Algorithms and Mathematics - Risi Kondor.mp3 71.59592529711378 66.00169779286927 137.59762308998305\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  85%|████████▍ | 11/13 [02:31<00:26, 13.30s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [65.87436332767402, <Segment(2687.02, 2752.89)>], 'SPEAKER_02': [1.7657045840405772, <Segment(2020.47, 2022.23)>], 'SPEAKER_00': [13.80305602716453, <Segment(2783.57, 2797.38)>], 'SPEAKER_03': [10.186757215619764, <Segment(2191.67, 2201.86)>]}\nSPEAKER_01 Flatiron Wide Algorithms and Mathematics - Bob Carpenter.mp3 65.87436332767402 2687.0203735144314 2752.8947368421054\nSPEAKER_00 Flatiron Wide Algorithms and Mathematics - Bob Carpenter.mp3 13.80305602716453 2783.5738539898134 2797.376910016978\nSPEAKER_03 Flatiron Wide Algorithms and Mathematics - Bob Carpenter.mp3 10.186757215619764 2191.6723259762307 2201.8590831918505\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file:  92%|█████████▏| 12/13 [03:03<00:18, 18.82s/it]","output_type":"stream"},{"name":"stdout","text":"{'SPEAKER_01': [0.5772495755518321, <Segment(635.679, 636.256)>], 'SPEAKER_00': [65.26315789473665, <Segment(1563.95, 1629.21)>]}\nSPEAKER_00 Cryo-EM Workshop - Roy Lederman.mp3 65.26315789473665 1563.947368421053 1629.2105263157896\n","output_type":"stream"},{"name":"stderr","text":"Saving audios for each speaker in each audio file: 100%|██████████| 13/13 [03:09<00:00, 14.58s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport scipy\nimport pandas as pd \nimport csv\nimport os\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import load_model\n\nimport glob\nimport shutil\nfrom pydub import AudioSegment\nimport librosa\n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:16:46.338581Z","iopub.execute_input":"2024-01-10T18:16:46.338977Z","iopub.status.idle":"2024-01-10T18:16:46.345514Z","shell.execute_reply.started":"2024-01-10T18:16:46.338931Z","shell.execute_reply":"2024-01-10T18:16:46.344633Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def extract_feature(file_name, **kwargs):\n    \"\"\"\n    Extract feature from audio file `file_name`\n        Features supported:\n            - MFCC (mfcc)\n            - Chroma (chroma)\n            - MEL Spectrogram Frequency (mel)\n            - Contrast (contrast)\n            - Tonnetz (tonnetz)\n    \"\"\"\n    mfcc = kwargs.get(\"mfcc\")\n    chroma = kwargs.get(\"chroma\")\n    mel = kwargs.get(\"mel\")\n    contrast = kwargs.get(\"contrast\")\n    tonnetz = kwargs.get(\"tonnetz\")\n\n    # Load the audio file with pydub\n    audio = AudioSegment.from_mp3(file_name)\n    sample_rate = audio.frame_rate\n\n    # Convert to NumPy array\n    audio_data = np.array(audio.get_array_of_samples())\n\n    if audio.channels == 2:  # Check if stereo and convert to mono\n        audio_data = audio_data.reshape((-1, 2))\n        audio_data = audio_data.mean(axis=1)\n        \n\n    audio_data = audio_data.astype(np.float32) / np.max(np.abs(audio_data))  # Normalize\n\n\n    # Feature extraction\n    result = np.array([])\n    if chroma or contrast or tonnetz:\n        stft = np.abs(librosa.stft(audio_data))\n    \n    if mfcc:\n        mfccs = np.mean(librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13).T, axis=0)\n        result = np.hstack((result, mfccs))\n\n    if chroma:\n        chroma_feature = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n        result = np.hstack((result, chroma_feature))\n\n    if mel:\n        mel_feature = np.mean(librosa.feature.melspectrogram(y=audio_data, sr=sample_rate).T, axis=0)\n        result = np.hstack((result, mel_feature))\n\n    if contrast:\n        contrast_feature = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n        result = np.hstack((result, contrast_feature))\n\n    if tonnetz:\n        tonnetz_feature = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(audio_data), sr=sample_rate).T, axis=0)\n        result = np.hstack((result, tonnetz_feature))\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:16:47.586823Z","iopub.execute_input":"2024-01-10T18:16:47.587239Z","iopub.status.idle":"2024-01-10T18:16:47.600227Z","shell.execute_reply.started":"2024-01-10T18:16:47.587202Z","shell.execute_reply":"2024-01-10T18:16:47.599015Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Gender Prediction\n\ndef preprocess_audio(speaker,audio_file):\n    # Extract the same features as we did for training\n    #audio = AudioSegment.from_mp3(\"/kaggle/working/\" + audio_file[-4] + speaker + \".mp3\")\n    file_path = \"/kaggle/working/\" + audio_file[:-4] + speaker + \".mp3\"\n    features = extract_feature(file_path, mel=True)\n    return features\n\ndef predict_gender(speaker,audio_file , model):\n    # Preprocess the file\n    features = preprocess_audio(speaker,audio_file)\n    # Reshape features to match the input shape of the model\n    features = np.reshape(features, (1, -1))\n    # Make a prediction\n    prediction = model.predict(features)[0]\n    # Interpret the result\n    if prediction <= 0.5:\n        return \"Female\",prediction\n    else:\n        return \"Male\",prediction\n\n# Load the model\nmodel = load_model(\"/kaggle/input/genderrec/model.h5\")\n\n\nfor file in os.listdir(audio_folder):\n    file_name = os.path.basename(file)\n    start = \"\\033[1m\"\n    end = \"\\033[0;0m\"\n    \n    print(\"Processing the conference : \" + start + file_name + end)\n    unique_speaker = unique_speakers[file_name]\n    Longest_segment = longest_segments_conferences[file_name]\n    speakers_duration = durations_conferences[file_name]\n\n    # Principal speaker is the one who talked the most\n    principal_speaker = max(speakers_duration, key=speakers_duration.get)\n    \n\n    #os.rename(\"/kaggle/working/\" + file_name + principal_speaker + \".mp3\", \"/kaggle/working/\" + file_name[-4] + principal_speaker + \".mp3\")\n   \n    gender = predict_gender(principal_speaker, file_name, model)\n    print(f\"The predicted gender for the principle speaker of this conference is: {gender}\")\n    print(f\"this speaker was interrupted by :{len(unique_speaker)-1}  persons\" )\n    for speaker in unique_speaker :\n        # Predict the gender\n        if speaker != principal_speaker:\n            #os.rename(\"/kaggle/working/\" + file_name + speaker + \".mp3\", \"/kaggle/working/\" + file_name[-4] + speaker + \".mp3\")                \n            try:\n                    gender = predict_gender(speaker, file_name, model)\n                    print(speaker + f\" The predicted gender for the interruptor is: {gender}\")\n            except FileNotFoundError:\n                    print(\"L'interruption était trop courte.\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:27:11.744333Z","iopub.execute_input":"2024-01-10T18:27:11.745123Z","iopub.status.idle":"2024-01-10T18:27:28.270590Z","shell.execute_reply.started":"2024-01-10T18:27:11.745075Z","shell.execute_reply":"2024-01-10T18:27:28.269382Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Processing the conference : \u001b[1mCryo-EM Workshop - Ayelet Heimowitz .mp3\u001b[0;0m\n1/1 [==============================] - 0s 122ms/step\nThe predicted gender for the principle speaker of this conference is: ('Female', array([0.02563079], dtype=float32))\nthis speaker was interrupted by :0  persons\nProcessing the conference : \u001b[1mCryo-EM Workshop - Jose Maria Carazo.mp3\u001b[0;0m\n1/1 [==============================] - 0s 55ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.875756], dtype=float32))\nthis speaker was interrupted by :1  persons\nL'interruption était trop courte.\nProcessing the conference : \u001b[1mFlatiron Wide Algorithms and Mathematics - Erik Thiede .mp3\u001b[0;0m\n1/1 [==============================] - 0s 57ms/step\nThe predicted gender for the principle speaker of this conference is: ('Female', array([0.10810881], dtype=float32))\nthis speaker was interrupted by :1  persons\n1/1 [==============================] - 0s 46ms/step\nSPEAKER_01 The predicted gender for the interruptor is: ('Male', array([0.92547244], dtype=float32))\nProcessing the conference : \u001b[1mFlatiron Wide Algorithms and Mathematics - Miles Stoudenmire .mp3\u001b[0;0m\n1/1 [==============================] - 0s 30ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.9660206], dtype=float32))\nthis speaker was interrupted by :5  persons\n1/1 [==============================] - 0s 40ms/step\nSPEAKER_01 The predicted gender for the interruptor is: ('Male', array([0.9688529], dtype=float32))\n1/1 [==============================] - 0s 41ms/step\nSPEAKER_03 The predicted gender for the interruptor is: ('Male', array([0.9645917], dtype=float32))\n1/1 [==============================] - 0s 44ms/step\nSPEAKER_00 The predicted gender for the interruptor is: ('Male', array([0.8897792], dtype=float32))\n1/1 [==============================] - 0s 43ms/step\nSPEAKER_05 The predicted gender for the interruptor is: ('Female', array([0.1475702], dtype=float32))\n1/1 [==============================] - 0s 57ms/step\nSPEAKER_04 The predicted gender for the interruptor is: ('Male', array([0.9670981], dtype=float32))\nProcessing the conference : \u001b[1mFlatiron Wide Algorithms and Mathematics - Ashley Villar .mp3\u001b[0;0m\n1/1 [==============================] - 0s 46ms/step\nThe predicted gender for the principle speaker of this conference is: ('Female', array([0.02939148], dtype=float32))\nthis speaker was interrupted by :0  persons\nProcessing the conference : \u001b[1mCryo-EM Workshop - Bridget Carragher .mp3\u001b[0;0m\n1/1 [==============================] - 0s 50ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.8419446], dtype=float32))\nthis speaker was interrupted by :0  persons\nProcessing the conference : \u001b[1mCryo-EM Workshop - Joakim Anden.mp3\u001b[0;0m\n1/1 [==============================] - 0s 57ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.9564641], dtype=float32))\nthis speaker was interrupted by :1  persons\nL'interruption était trop courte.\nProcessing the conference : \u001b[1mFlatiron Wide Algorithms and Mathematics - Shirley Ho.mp3\u001b[0;0m\n1/1 [==============================] - 0s 55ms/step\nThe predicted gender for the principle speaker of this conference is: ('Female', array([0.02505006], dtype=float32))\nthis speaker was interrupted by :3  persons\n1/1 [==============================] - 0s 49ms/step\nSPEAKER_01 The predicted gender for the interruptor is: ('Male', array([0.73743623], dtype=float32))\n1/1 [==============================] - 0s 42ms/step\nSPEAKER_02 The predicted gender for the interruptor is: ('Male', array([0.99993527], dtype=float32))\n1/1 [==============================] - 0s 49ms/step\nSPEAKER_00 The predicted gender for the interruptor is: ('Male', array([0.9123321], dtype=float32))\nProcessing the conference : \u001b[1mCryo-EM Workshop - Erik Lindahl .mp3\u001b[0;0m\n1/1 [==============================] - 0s 52ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.93589956], dtype=float32))\nthis speaker was interrupted by :1  persons\nL'interruption était trop courte.\nProcessing the conference : \u001b[1mCryo-EM Workshop - Joachim Frank.mp3\u001b[0;0m\n1/1 [==============================] - 0s 54ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.9060482], dtype=float32))\nthis speaker was interrupted by :1  persons\nL'interruption était trop courte.\nProcessing the conference : \u001b[1mFlatiron Wide Algorithms and Mathematics - Risi Kondor.mp3\u001b[0;0m\n1/1 [==============================] - 0s 58ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.95228237], dtype=float32))\nthis speaker was interrupted by :1  persons\n1/1 [==============================] - 0s 52ms/step\nSPEAKER_01 The predicted gender for the interruptor is: ('Male', array([0.93438816], dtype=float32))\nProcessing the conference : \u001b[1mFlatiron Wide Algorithms and Mathematics - Bob Carpenter.mp3\u001b[0;0m\n1/1 [==============================] - 0s 51ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.7266128], dtype=float32))\nthis speaker was interrupted by :3  persons\nL'interruption était trop courte.\n1/1 [==============================] - 0s 46ms/step\nSPEAKER_00 The predicted gender for the interruptor is: ('Male', array([0.949453], dtype=float32))\n1/1 [==============================] - 0s 43ms/step\nSPEAKER_03 The predicted gender for the interruptor is: ('Male', array([0.92625886], dtype=float32))\nProcessing the conference : \u001b[1mCryo-EM Workshop - Roy Lederman.mp3\u001b[0;0m\n1/1 [==============================] - 0s 50ms/step\nThe predicted gender for the principle speaker of this conference is: ('Male', array([0.90477705], dtype=float32))\nthis speaker was interrupted by :1  persons\nL'interruption était trop courte.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(Diarizations['Cryo-EM Workshop - Jose Maria Carazo.mp3'])","metadata":{"execution":{"iopub.status.busy":"2024-01-10T18:24:10.325745Z","iopub.execute_input":"2024-01-10T18:24:10.326548Z","iopub.status.idle":"2024-01-10T18:24:10.332555Z","shell.execute_reply.started":"2024-01-10T18:24:10.326510Z","shell.execute_reply":"2024-01-10T18:24:10.331563Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"[ 00:00:08.191 -->  00:00:57.631] A SPEAKER_00\n[ 00:00:58.191 -->  00:02:06.748] B SPEAKER_00\n[ 00:02:07.292 -->  00:02:18.853] C SPEAKER_00\n[ 00:02:18.938 -->  00:03:11.553] D SPEAKER_00\n[ 00:03:13.081 -->  00:04:31.078] E SPEAKER_00\n[ 00:04:31.298 -->  00:04:49.567] F SPEAKER_00\n[ 00:04:50.059 -->  00:04:55.000] G SPEAKER_00\n[ 00:04:55.696 -->  00:05:25.865] H SPEAKER_00\n[ 00:05:26.867 -->  00:06:38.395] I SPEAKER_00\n[ 00:06:38.752 -->  00:06:56.952] J SPEAKER_00\n[ 00:06:57.444 -->  00:06:59.516] K SPEAKER_00\n[ 00:07:00.331 -->  00:07:31.485] L SPEAKER_00\n[ 00:07:31.723 -->  00:07:32.470] M SPEAKER_00\n[ 00:07:32.589 -->  00:07:47.003] N SPEAKER_00\n[ 00:07:48.174 -->  00:07:51.808] O SPEAKER_00\n[ 00:07:52.351 -->  00:07:55.543] P SPEAKER_00\n[ 00:07:56.069 -->  00:08:05.424] Q SPEAKER_00\n[ 00:08:05.831 -->  00:08:12.385] R SPEAKER_00\n[ 00:08:12.860 -->  00:08:57.954] S SPEAKER_00\n[ 00:08:59.482 -->  00:09:25.679] T SPEAKER_00\n[ 00:09:25.916 -->  00:10:03.777] U SPEAKER_00\n[ 00:10:04.915 -->  00:10:07.478] V SPEAKER_00\n[ 00:10:08.005 -->  00:10:14.745] W SPEAKER_00\n[ 00:10:15.000 -->  00:11:00.432] X SPEAKER_00\n[ 00:11:01.943 -->  00:11:09.414] Y SPEAKER_00\n[ 00:11:11.196 -->  00:12:46.001] Z SPEAKER_00\n[ 00:12:47.088 -->  00:14:24.490] AA SPEAKER_00\n[ 00:14:25.543 -->  00:14:29.482] AB SPEAKER_00\n[ 00:14:29.499 -->  00:14:30.959] AC SPEAKER_00\n[ 00:14:31.298 -->  00:14:42.028] AD SPEAKER_00\n[ 00:14:42.504 -->  00:14:43.879] AE SPEAKER_00\n[ 00:14:44.371 -->  00:14:47.767] AF SPEAKER_00\n[ 00:14:48.259 -->  00:17:50.704] AG SPEAKER_00\n[ 00:17:31.196 -->  00:17:31.570] AH SPEAKER_01\n[ 00:17:50.942 -->  00:17:53.930] AI SPEAKER_00\n[ 00:17:54.354 -->  00:17:55.067] AJ SPEAKER_00\n[ 00:17:55.373 -->  00:17:56.748] AK SPEAKER_00\n[ 00:17:57.275 -->  00:18:00.365] AL SPEAKER_00\n[ 00:18:00.874 -->  00:19:41.417] AM SPEAKER_00\n[ 00:19:42.011 -->  00:20:24.168] AN SPEAKER_00\n[ 00:20:25.101 -->  00:20:25.882] AO SPEAKER_00\n[ 00:20:26.494 -->  00:20:34.049] AP SPEAKER_00\n[ 00:20:34.320 -->  00:20:52.079] AQ SPEAKER_00\n[ 00:20:53.030 -->  00:21:19.176] AR SPEAKER_00\n[ 00:21:20.432 -->  00:21:20.959] AS SPEAKER_00\n[ 00:21:22.572 -->  00:21:26.086] AT SPEAKER_00\n[ 00:21:26.799 -->  00:21:31.213] AU SPEAKER_00\n[ 00:21:31.926 -->  00:21:42.436] AV SPEAKER_00\n[ 00:21:44.286 -->  00:22:03.573] AW SPEAKER_00\n[ 00:22:04.728 -->  00:22:05.373] AX SPEAKER_00\n[ 00:22:06.001 -->  00:23:10.195] AY SPEAKER_00\n[ 00:23:10.280 -->  00:23:27.342] AZ SPEAKER_00\n[ 00:23:27.733 -->  00:23:33.947] BA SPEAKER_00\n[ 00:23:35.254 -->  00:25:05.203] BB SPEAKER_00\n[ 00:25:05.831 -->  00:25:19.940] BC SPEAKER_00\n[ 00:25:20.908 -->  00:25:46.578] BD SPEAKER_00\n[ 00:25:48.022 -->  00:26:05.118] BE SPEAKER_00\n[ 00:26:06.392 -->  00:26:07.376] BF SPEAKER_00\n[ 00:26:07.665 -->  00:26:41.061] BG SPEAKER_00\n[ 00:26:41.740 -->  00:26:42.928] BH SPEAKER_00\n[ 00:26:42.996 -->  00:27:02.130] BI SPEAKER_00\n[ 00:27:03.930 -->  00:27:15.084] BJ SPEAKER_00\n[ 00:27:15.288 -->  00:27:42.860] BK SPEAKER_00\n[ 00:27:44.151 -->  00:27:52.945] BL SPEAKER_00\n[ 00:27:54.949 -->  00:28:20.365] BM SPEAKER_00\n[ 00:28:21.994 -->  00:28:23.098] BN SPEAKER_00\n[ 00:28:24.677 -->  00:29:25.865] BO SPEAKER_00\n[ 00:29:27.529 -->  00:31:11.638] BP SPEAKER_00\n[ 00:29:35.577 -->  00:29:35.747] BQ SPEAKER_01\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}